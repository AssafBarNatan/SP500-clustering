{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates our data retrieval process. We cover the main constructs and functionality of our `retrieval` module under the package `data_pipeline` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `DataBank` Construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop (D)\\Erdos Intitute\\Project\\SP500-clustering\\notebooks\\data_retrieval.ipynb Cell 4\u001b[0m line \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Desktop%20%28D%29/Erdos%20Intitute/Project/SP500-clustering/notebooks/data_retrieval.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_pipeline\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mretrieval\u001b[39;00m \u001b[39mimport\u001b[39;00m DataBank, download_adj_close\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from ..data_pipeline.retrieval import DataBank, download_adj_close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `DataBank` collects all the tickers from the Wikipedia article on the list of S&P 500 companies. It has methods for downloading all the tickers, as well as organizing the tickers by GCIS Sector and GCIS Sub-Industry. Let's initialize an instance of our `DataBank` and collect tickers. (As a sanity check, we compute the length of the list of tickers we obtain.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bank = DataBank()\n",
    "tickers = data_bank.get_tickers()\n",
    "\n",
    "len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tickers by Sector and Sub-Industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tickers can also be obtained by sector and sub-industry. In particular, the `DataBank` object has methods for constructing ticker-to-sector maps and ticker-to-subindustry maps. Here, the terms 'sector' and 'sub-industry' refer to the [GICS](https://en.wikipedia.org/wiki/Global_Industry_Classification_Standard) sectors and sub-industries. \n",
    "\n",
    "These methods are particularly useful when one would like to cluster particular tickers in accordance with their GICS classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bank.get_sectors_list()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bank.get_subind_list()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(islice(data_bank.ticker_to_sector_map().items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(islice(data_bank.ticker_to_subind_map().items(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download our data from [Yahoo! Finance](https://finance.yahoo.com/) API using the [`yfinance` library](https://github.com/ranaroussi/yfinance). Our primary data of interest will be the adjusted closing prices for the various tickers considered. \n",
    "\n",
    "We constructed functions for downloading historical data that are built on top of `yfinance`. We specifiy:\n",
    "* A list of tickers,\n",
    "* A start date and an end date (instead of a period),\n",
    "* An interval (e.g. 1 month for monthly data vs 1 day for daily data, etc),\n",
    "and obtain a dataframe whose columns are the tickers, and whose index is the date.\n",
    "\n",
    "We have monthly data and daily data for two years, ending in end of November 2023, saved in cold storage as pickle files in the local folder [`data`](../../data/) in this repository.\n",
    "\n",
    "In this notebook, we demonstrate the functionality with 3 days of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '2023-28-11'\n",
    "end = TODAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_closing_prices = download_adj_close(tickers, start, save_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_closing_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data processing pipeline is simple:\n",
    "* We take the adjusted close price data previously obtained as a dataframe from our retrieval process.\n",
    "* We feed this dataframe to a `ClusterInput` class as an attribute, along with an optional second attribute `transform`. This second attribute is a function that will transform the multivariate time series into a multivariate time series that is more suitable for clustering purposes. For instance, we could normalize the time series using the $\\ell_2$-norm. We would also take the series of returns as our clustering input. Alternatively, we could take the return of return series.\n",
    "* The resulting `ClusterInput` object then has a `df` attribute representing the transposed transformed time series as a dataframe. The latter can readily be used as an input (features) for various standard clustering models that are part of standard libraries such as `scikit-learn` and `tslearn`.\n",
    "\n",
    "Note that this input can be further by processing by the standard `preprocessing` classes of the aforementioned libraries. \n",
    "\n",
    "In what follows, we offer two examples: one using `KMeans` from `scikit-learn` and one using `TimeSeriesKMeans` from `tslearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Input Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.processing import ClusterInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_input = ClusterInput(adj_closing_prices).df\n",
    "clustering_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Preprocessing (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "features_sklearn = RobustScaler().fit_transform(clustering_input)\n",
    "features_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "\n",
    "features_tslearn = TimeSeriesScalerMinMax().fit_transform(clustering_input)\n",
    "features_tslearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Our `ClusterInput` To Models From Standard Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate this with preprocessing (on `features_<library_name>`) and without preprocessing (on `cluster_input`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "KMeans().fit(features_sklearn).labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans().fit(clustering_input).labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "TimeSeriesKMeans().fit(features_tslearn).labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeSeriesKMeans().fit(clustering_input).labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
